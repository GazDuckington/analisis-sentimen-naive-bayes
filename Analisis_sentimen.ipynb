{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt', download_dir='NLTK_DATA')\n",
    "# nltk.download('stopwords', download_dir='NLTK_DATA')\n",
    "nltk.data.path.append(\"NLTK_DATA\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from processing import freqs, normalisasi\n",
    "from sentiment import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPARASI *TRAINING DATASET*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: pindah ke sqlite\n",
    "dataset = pd.read_csv(\"dataset/training_dataset.csv\")\n",
    "all_pos = dataset[dataset['label'] == 1]\n",
    "all_neg = dataset[dataset['label'] == 0]\n",
    "\n",
    "# * Training data: 80% pertama\n",
    "# * Testing data: 20% terakhir\n",
    "train_pos = all_pos[:int(len(all_pos)*0.8)]\n",
    "train_neg = all_neg[:int(len(all_neg)*0.8)]\n",
    "test_pos = all_pos[-int(len(all_pos)*0.2):]\n",
    "test_neg = all_neg[-int(len(all_neg)*0.2):]\n",
    "\n",
    "train_x = train_pos.append(train_neg, ignore_index=True)\n",
    "test_x = test_pos.append(test_neg, ignore_index=True)\n",
    "\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld = len(dataset)\n",
    "trp = len(train_pos)\n",
    "tng = len(train_neg)\n",
    "tsp = len(test_pos)\n",
    "tsn = len(test_neg)\n",
    "print(f\"dataset: {ld} \\n train pos: {trp} \\n train neg: {tng} \\n test pos: {tsp} \\n test neg: {tsn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PELATIHAN *MODEL CLASSIFIER*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_x['text'].tolist()\n",
    "y = train_y.tolist()\n",
    "kamus_frek = kamus_freq(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in kamus_frek.keys():\n",
    "    print(kamus_frek.get(x, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logprior\n",
    "$D$ = jumlah dokumen <br>\n",
    "$D_{pos}$ = jumlah dokumen positif <br>\n",
    "$D_{neg}$ = jumlah dokumen negatif <br>\n",
    "### probabilitas dokumen positif\n",
    "$P(D_{pos}) = \\frac{D_{pos}}{D}$\n",
    "### probabilitas dokumen negatif\n",
    "$P(D_{neg}) = \\frac{D_{neg}}{D}$\n",
    "### probabilitas prior\n",
    "${logprior} = log \\left( \\frac{P(D_{pos})}{P(D_{neg})} \\right) = log \\left( \\frac{D_{pos}}{D_{neg}} \\right)$\n",
    "<br> disederhanakan, menjadi: <br>\n",
    "$logprior = P(D_{pos}) - P(D_{neg})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loglikelihood\n",
    "$W$ = probabilitas positif/negatif kata <br>\n",
    "$freq$ = frekuensi kata unik yang bernilai positif/negatif <br>\n",
    "$N$ =  jumlah total kata positif/negatif <br>\n",
    "$V$ = jumlah total kata unik (panjang dokumen frekuensi) <br>\n",
    "### probabilitas kata positif\n",
    "$P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}$\n",
    "### probabilitas kata negatif\n",
    "$P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V}$\n",
    "### likelihood kata\n",
    "$\\text{loglikelihood} = \\log \\left(\\frac{P(W_{pos})}{P(W_{neg})} \\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logprior, loglikelihood = train_nbc(kamus_frek, train_y)\n",
    "logprior = hitungPrior(kamus_frek, train_y)\n",
    "loglikelihood = hitungLikelihood(kamus_frek)\n",
    "# print(logprior)\n",
    "# print(loglikelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HASIL PENGUJIAN MODEL\n",
    "- data *training*\n",
    "- data *testing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8455598455598455"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = test_nbc(test_x['text'].tolist(), test_y.tolist(), logprior, loglikelihood)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UJI KESELURUHAN *DATASET*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = test_nbc(dataset['text'], dataset['label'], logprior, loglikelihood)\n",
    "data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *CONFUSION MATRIX*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## metrik-metrik\n",
    "-   TP = True Positive\n",
    "-   TN = True Negative\n",
    "-   FP = False Positive\n",
    "-   FN = False Negative\n",
    "- Populasi = TP+TN+FP+FN\n",
    "<hr>\n",
    "\n",
    "### akurasi\n",
    "\n",
    "jumlah prediksi yang benar. tidak dianjurkan sebagai metrik kunci. <br>\n",
    "$\\frac{TP+TN}{Populasi}$\n",
    "\n",
    "### miss\n",
    "\n",
    "jumlah prediksi yang salah <br>\n",
    "$\\frac{FP+FN}{Populasi}$\n",
    "\n",
    "### presisi\n",
    "\n",
    "persentase positif yang diprediksi secara akurat. jika FP = 0, maka nilai presisi 100%. <br>\n",
    "$\\frac{TP}{TP+FP}$\n",
    "\n",
    "### sensitifitas\n",
    "\n",
    "nilai positif sesungguhnya <br>\n",
    "$\\frac{TP}{TP+FN}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdc = pd.DataFrame()\n",
    "pdc = dataset\n",
    "pdc['label_prediksi'] = pdc['text'].apply(lambda x: predict_nbc(x, logprior, loglikelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Do more testing with thresholds values from 0, 0.5, 0.8, 0.9, 1\n",
    "# * to note;\n",
    "# treshold = -0.03272669350255697\n",
    "# * default treshold is 0\n",
    "def toLabel(x):\n",
    "   if x > 0:\n",
    "       return 1\n",
    "   elif x < 0:\n",
    "       return 0\n",
    "   else:\n",
    "       return \"n\"\n",
    "pdc['prediksi'] = pdc['label_prediksi'].apply(lambda x: toLabel(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = logprior\n",
    "def toLabel(x):\n",
    "   if x > tr:\n",
    "       return 1\n",
    "   elif x < tr:\n",
    "       return 0\n",
    "   else:\n",
    "       return \"n\"\n",
    "pdc['prediksi_tr'] = pdc['label_prediksi'].apply(lambda x: toLabel(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = pd.crosstab(pdc['label'], pdc['prediksi'], rownames=['Actual'], colnames=['Predicted'])\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = 647+555+14+84\n",
    "a = (647+555)/pop\n",
    "m = (14+84)/pop\n",
    "p = 555/(555+14)\n",
    "s = 555/(555+84)\n",
    "print(f\"pop={pop}\\nakurasi:{a}, miss:{m}, presisi: {p}, sensi: {s}.\\n F1:{2*(p*s)/(p+s)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0\n",
    "for index, row in pdc.iterrows():\n",
    "    if row['prediksi'] == row['label']:\n",
    "        acc+=1\n",
    "\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf2 = pd.crosstab(pdc['label'], pdc['prediksi_tr'], rownames=['Actual'], colnames=['Predicted'])\n",
    "conf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2 = (647+556)/1300\n",
    "m2 = (14+83)/1300\n",
    "p2 = 556/(556+14)\n",
    "s2 = 556/(556+83)\n",
    "print(f\"akurasi:{a2}, miss:{m2}, presisi: {p2}, sensi: {s2}.\\n F1:{2*(p2*s2)/(p2+s2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sn.heatmap(conf_matrix, annot=True, fmt='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.heatmap(conf2, annot=True, fmt='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UJI CLASSIFIER PADA ARTIKEL BERITA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "berita = open('berita.txt', 'r')\n",
    "berita = berita.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beritanormal = normalisasi(berita)\n",
    "frekberita = freqs(beritanormal)\n",
    "# frekberita"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SENTIMEN ARTIKEL KESELURUHAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_nbc(berita, logprior, loglikelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SENTIMEN ARTIKEL PER_KALIMAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "x = pd.DataFrame()\n",
    "kalimat = sent_tokenize(berita)\n",
    "x['kalimat'] = kalimat\n",
    "x['kalimat_normal'] = x['kalimat'].apply(lambda x: normalisasi(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['label_prediksi'] = x['kalimat'].apply(lambda x: predict_nbc(x, logprior, loglikelihood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['prediksi'] = x['label_prediksi'].apply(lambda x: toLabel(x))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SENTIMEN ARTIKEL PER_KATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "berita_normal = normalisasi(berita)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * predict_nbc per-kata\n",
    "def predict_perkata(text, logpri, loglik):\n",
    "    l = []\n",
    "    for w in text:\n",
    "        x = predict_nbc(w, logpri, loglik)\n",
    "        l.append({\n",
    "            \"kata\":w,\n",
    "            \"skor\":x\n",
    "            })\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasil = predict_perkata(berita_normal, logprior, loglikelihood)\n",
    "type(hasil[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "keys = hasil[0].keys()\n",
    "\n",
    "with open('demo-kata.csv', 'w', newline='') as file:\n",
    "    dict_writer = csv.DictWriter(file, keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(hasil)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('analisis-sentimen-naive-bayes-TsMnn7d4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ae47f95440d1b9e894a7616647b08b93b0b0191ab4551829de60d6a9373aad9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
